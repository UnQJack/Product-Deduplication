import pandas as pd
from fuzzywuzzy import fuzz 
from fuzzywuzzy.process import extractOne
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN
from scipy.spatial.distance import cosine

# citirea fisierului parquet
df = pd.read_parquet("veridion_product_deduplication_challenge.snappy.parquet", engine="pyarrow")

# noralizarea textului
def normalize_text(text):
    if pd.isna(text):
        return ""
    return text.lower().strip().replace(".", "").replace(",", "")

# normalizarea numelor produselor
df["normalized_name"] = df["product_name"].astype(str).apply(normalize_text)

# similaritatea pentru detecția duplicatelor
vectorizer = TfidfVectorizer(stop_words="english")
name_matrix = vectorizer.fit_transform(df["normalized_name"])

#similaritatea între documente
similarity_matrix = name_matrix * name_matrix.T

# gruparea produselor similare
clustering = DBSCAN(eps=0.3, min_samples=2, metric="cosine").fit(name_matrix.toarray())
df["cluster"] = clustering.labels_

# duplicatele
def merge_products(group):
    return pd.Series({
        "product_name": group["product_name"].iloc[0],
        "brand": group["brand"].mode()[0] if not group["brand"].mode().empty else group["brand"].iloc[0],
        "description": max(group["description"], key=len) if group["description"].notna().any() else None,
        "price_min": group["price"].min(),
        "price_max": group["price"].max(),
        "price_avg": group["price"].mean(),
        "image_urls": list(set(group["image_url"].dropna())),
        "sources": list(set(group["source"].dropna()))
    })

deduplicated_df = df.groupby("cluster").apply(merge_products).reset_index(drop=True)

# afiseaza rezultatul
deduplicated_df.to_csv("deduplicated_products.csv", index=False)
print("Deduplication complete. Saved as 'deduplicated_products.csv'.")
